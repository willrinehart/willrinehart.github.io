<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> Is Facebook Now Over-moderating Content? | Will Rinehart</title>
  <meta name="description" content="My name is William Rinehart, but you can call me Will. Currently, I’m a Senior Research Fellow at the Center for Growth and Opportunity (CGO) at Utah State. Previously, I was the Director of Technology and Innovation Policy at the American Action Forum. In my day to day job, I specialize on the public policy of telecommunication, the Internet, platform companies, and AI, with a focus on emerging technologies and innovation. Fundamentally, I am interested in expanding human flourishing, reforming government to make it efficient, and learning.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:title" content="Is Facebook Now Over-moderating Content?" />
<meta property="og:description" content="Reading professor Siva Vaidhyanathan’s recent op-ed in the New York Times, one could reasonably assume that Facebook is now seriously tackling the enormous problem of dangerous information. In detailing his takeaways from a recent hearing with Facebook’s COO Sheryl Sandberg and Twitter CEO Jack Dorsey, Vaidhyanathan explained,



But it could be that, in their zeal to trapple down criticism from all sides, Facebook instead has corrected too far and is now over-moderating. The fundamental problem is that it is nearly impossible to know the true amount of disinformation on a platform. For one, there is little agreement on what kind of content needs to be policed. It is doubtful everyone would agree what constitutes fake news and separates it from disinformation or propaganda and how all of that differs from hate speech. But more fundamentally, even if everyone agreed to what should be taken down, it is still not clear that algorithmic filtering methods would be able to perfectly approximate that." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://example.com/2018/is-facebook-now-over-moderating-content/" />
<meta property="article:published_time" content="2018-09-10T14:30:32+00:00" />
<meta property="article:modified_time" content="2018-09-10T14:30:32+00:00" />

  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Is Facebook Now Over-moderating Content?"/>
<meta name="twitter:description" content="Reading professor Siva Vaidhyanathan’s recent op-ed in the New York Times, one could reasonably assume that Facebook is now seriously tackling the enormous problem of dangerous information. In detailing his takeaways from a recent hearing with Facebook’s COO Sheryl Sandberg and Twitter CEO Jack Dorsey, Vaidhyanathan explained,



But it could be that, in their zeal to trapple down criticism from all sides, Facebook instead has corrected too far and is now over-moderating. The fundamental problem is that it is nearly impossible to know the true amount of disinformation on a platform. For one, there is little agreement on what kind of content needs to be policed. It is doubtful everyone would agree what constitutes fake news and separates it from disinformation or propaganda and how all of that differs from hate speech. But more fundamentally, even if everyone agreed to what should be taken down, it is still not clear that algorithmic filtering methods would be able to perfectly approximate that."/>

  
  
    
  
  
  <link rel="stylesheet" href="https://example.com/css/style-white.css">
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="https://example.com/images/favicon.ico" />

  
  
  
</head>
<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

    <header id="header">
  <a href="https://example.com">
  
    <div id="logo" style="background-image: url(https://example.com/images/logo.png)"></div>
  
  <div id="title">
    <h1>Will Rinehart</h1>
  </div>
  </a>
  <div id="nav">
    <ul>
      <li class="icon">
        <a href="#"><i class="fas fa-bars fa-2x"></i></a>
      </li>
       
        <li><a href="/">Home</a></li>
       
        <li><a href="/posts">Blog</a></li>
       
        <li><a href="/publications">Publications</a></li>
       
        <li><a href="/media">Media</a></li>
       
        <li><a href="/about">About</a></li>
      
    </ul>
  </div>
</header>
  

    
<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <div class="content" itemprop="articleBody">
  
    <p><!-- raw HTML omitted -->Reading professor Siva Vaidhyanathan’s <!-- raw HTML omitted --><a href="https://www.nytimes.com/2018/09/05/opinion/facebook-sandberg-congress.html"><!-- raw HTML omitted -->recent op-ed<!-- raw HTML omitted --></a> <!-- raw HTML omitted -->in the New York Times, one could reasonably assume that Facebook is now seriously tackling the enormous problem of dangerous information. In detailing his takeaways from a recent hearing with Facebook’s COO Sheryl Sandberg and Twitter CEO Jack Dorsey, Vaidhyanathan explained,<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted -->But it could be that, in their zeal to trapple down criticism from all sides, Facebook instead has corrected too far and is now over-moderating. The fundamental problem is that it is nearly impossible to know the true amount of disinformation on a platform. For one, there is little agreement on what kind of content needs to be policed. It is doubtful everyone would agree what constitutes fake news and separates it from disinformation or propaganda and how all of that differs from hate speech. But more fundamentally, even if everyone agreed to what should be taken down, it is still not clear that algorithmic filtering methods would be able to perfectly approximate that.</p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted -->Detecting content that violates a hate speech code or a disinformation standard leads into a massive operationalization problem. A company like Facebook isn’t going to be perfect. It could produce a detection regime that was either underbroad or overbroad. It is of course only minimal evidence, but I have been seeing a lot of my friends on Facebook post about how their own posts have been taken down and it was clear they were non-political. <!-- raw HTML omitted --><br>
<!-- raw HTML omitted -->Over-moderation could explain why many conservatives have been worried about Twitter and Facebook engaging in soft censorship. Paula Bolyard made a convincing case in <!-- raw HTML omitted --><a href="https://www.washingtonpost.com/opinions/i-wrote-the-article-about-media-bias-in-google-searches-regulation-isnt-the-answer/2018/08/29/15bdaae2-abaa-11e8-8f4b-aee063e14538_story.html?utm_term=.b89b16abb402"><!-- raw HTML omitted -->the Washington Post<!-- raw HTML omitted --></a><!-- raw HTML omitted -->,<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted -->Bolyard hints at one of the biggest problems in the conversation today. Users cannot peer behind the veil and are thus forced to impute intentions about how the network operates in practice. Here is how Sarah Myers West, a postdoc researcher at the AI Now Institute, <!-- raw HTML omitted --><a href="http://journals.sagepub.com/doi/full/10.1177/1461444818773059"><!-- raw HTML omitted -->described<!-- raw HTML omitted --></a> <!-- raw HTML omitted -->the process,    <!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted -->West goes on to cite a <!-- raw HTML omitted --><a href="https://dl.acm.org/citation.cfm?id=2702556"><!-- raw HTML omitted -->study of moderation efforts<!-- raw HTML omitted --></a><!-- raw HTML omitted -->, which found that users thought Facebook was “powerful, perceptive, and ultimately unknowable.” Both Vaidhyanathan and Bolyard could pushing similar folk theories. They are both astute in their comments and offer a lot to consider, but everyone in this discussion, including the operators at Facebook and Twitter, is hobbled by a fundamental knowledge problem.<!-- raw HTML omitted --><br>
<!-- raw HTML omitted -->Still, each platform has to create its own means of detecting this content, which will need to conform to the specifics of the platform. Evelyn Douek’s <!-- raw HTML omitted --><a href="https://www.lawfareblog.com/senate-hearing-social-media-and-foreign-influence-operations-progress-theres-long-way-go"><!-- raw HTML omitted -->report on the Senate Hearing<!-- raw HTML omitted --></a><!-- raw HTML omitted -->, which you should absolutely go read, helps to fill out some of the details on this point,<!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted -->Dorsey seems to set up a comparison between the two companies. Facebook’s method of detecting nefarious content deals with the profile, as an authenticated person, in relation to the content that is shared. Twitter, on the other hand, is looking for people to game the system in the “shared spaces [on Twitter] where anyone can interject themselves.” It might be a misread, but Dorsey suggests that Twitter is emphasizing the actions of users, which would lead to a more structural approach. <!-- raw HTML omitted --><br>
<!-- raw HTML omitted -->It goes without saying that Facebook’s social network is different from Twitter’s, leading to different approaches in moderation. Facebook creates dyadic connections. The relationships on Facebook run both ways. Becoming friends means we are in a mutual relationship. Twitter, however, allows for people to follow others without reciprocity. The result are distinct network structures. Pew, for example, was able to distinguish between <!-- raw HTML omitted --><a href="http://www.pewinternet.org/2014/02/20/mapping-twitter-topic-networks-from-polarized-crowds-to-community-clusters/"><!-- raw HTML omitted -->six different broad structures<!-- raw HTML omitted --></a><!-- raw HTML omitted -->, including polarized crowds, tight crowds, brand clusters, community clusters, broadcast networks, and support networks. Combined, these features make it difficult for both researchers and operators to understand the scope of the problem and how solutions are working, or not working. <!-- raw HTML omitted --><br>
<!-- raw HTML omitted -->So what are the broad incentives pushing platforms to either over-moderate or under-moderate content? Here is what I could come up with:  <!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p><!-- raw HTML omitted -->Content moderation at scale is difficult. And messy. In creating a technical regime to deal with this problem, we shouldn’t expect platforms to get it perfect. While many have criticized platforms for under-moderation, they might now being over-moderating. Still, there is a massive knowledge problem in trying to understand if the current level of moderation is optimal.    <!-- raw HTML omitted --></p>
  
  </div>
</article>


    <footer id="footer">
  <div class="footer-left">
    Copyright  &copy; 2020  Will Rinehart | economist, speaker, and analyst of tech policy 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/posts">Blog</a></li>
         
        <li><a href="/publications">Publications</a></li>
         
        <li><a href="/media">Media</a></li>
         
        <li><a href="/about">About</a></li>
        
      </ul>
    </nav>
  </div>
</footer>


  </div>
  
</body>

<link rel="stylesheet" href=/lib/font-awesome/css/all.min.css>
<script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>
</html>
